This document was generated by an AI.

# LLM.py 코드 설명서

## 1. 소개

이 문서는 `LLM.py` 파일에 포함된 Python 코드의 기능, 구조 및 사용법에 대해 설명합니다. 이 스크립트는 LangChain 및 LangGraph 라이브러리를 활용하여 LLM 기반의 에이전트 워크플로우를 구축하고 관리하는 것을 목표로 합니다.

## 2. 주요 기능

*   **환경 변수 관리:** `.env` 파일 로드 및 LangSmith 추적 활성화
*   **LLM 통합:** Google Generative AI 모델 (Gemini) 연동 및 도구 바인딩
*   **상태 관리:** 에이전트의 메시지, 계획, 현재 단계 등을 포함하는 `AgentState` 정의
*   **도구 정의:** `log_reasoning`, `generate_code_draft`, `list_project_structure`, `read_file`, `write_code_to_file`, `append_to_file`, `execute_command` 등 다양한 유틸리티 도구 제공
*   **MCP (Multi-modal Conversational Platform) 연동:** 외부 도구 및 서비스와의 통합 지원 (선택 사항)
*   **그래프 기반 워크플로우:** LangGraph를 이용한 에이전트의 순차적 실행 (Planner -> Executor)
*   **구조화된 출력:** Pydantic 모델을 이용한 LLM의 응답 형식 지정 (예: `PlanSchema`, `ReviewReport`)

## 3. 클래스 및 함수 설명

### 3.1. 설정 (Config)

*   `Config` 클래스: 모델 이름, 최대 재시도 횟수, 로깅 레벨, MCP 설정 경로 등 전역 설정을 관리합니다.

### 3.2. 상태 정의 (AgentState)

*   `AgentState` (TypedDict): 에이전트 워크플로우의 상태를 정의합니다. 포함되는 항목은 다음과 같습니다:
    *   `messages`: 대화 기록 (BaseMessage 리스트)
    *   `plan`: 실행할 단계별 계획 (문자열 리스트)
    *   `current_step_index`: 현재 실행 중인 계획 단계의 인덱스
    *   `final_report`: 최종 결과 보고서 (선택 사항, dict)

### 3.3. 스키마 정의 (Pydantic Models)

*   `PlanSchema`: LLM이 생성할 실행 계획의 구조를 정의합니다. `steps` (실행할 단계 목록) 필드를 포함합니다.
*   `ReviewReport`: 작업의 성공/실패 여부와 요약 정보를 담는 스키마입니다.

### 3.4. 도구 (Tools)

*   `log_reasoning(reasoning: str)`: 분석 및 검토 내용을 기록합니다.
*   `generate_code_draft(code_snippet: str, description: str)`: 코드 초안을 생성하고 검토를 위해 반환합니다. 실제 파일 저장 기능은 포함하지 않습니다.
*   `list_project_structure(root_path: str = ".", max_depth: int = 3)`: 프로젝트의 파일 및 디렉토리 구조를 조회합니다. `.git`, `.venv` 등 일반적인 무시 대상 디렉토리를 제외합니다.
*   `read_file(file_path: str)`: 지정된 경로의 파일 내용을 읽어옵니다 (최대 10,000자).
*   `write_code_to_file(file_path: str, content: str)`: 지정된 경로에 파일을 작성하거나 덮어씁니다. 디렉토리가 없으면 자동으로 생성합니다.
*   `append_to_file(file_path: str, content: str)`: 기존 파일의 끝에 내용을 추가합니다.
*   `execute_command(command: str)`: 쉘 명령어를 실행하고 그 결과를 반환합니다.

### 3.5. 매니저 클래스

*   `MCPManager`: MCP (Multi-modal Conversational Platform)와의 연동을 관리합니다. 설정 파일(`config/mcp.json`)을 로드하여 MCP 서버에 연결하고 도구를 로드합니다.
*   `LLMManager`:
    *   `initialize()`: Google Generative AI 모델 (`ChatGoogleGenerativeAI`)을 초기화하고, 제공된 도구들과 바인딩하여 `llm_with_tools` 객체를 생성합니다.
    *   `invoke()`: 설정된 모드 (`tools`, `structured`, 기본)에 따라 LLM을 호출하고 결과를 반환합니다. 구조화된 출력을 위해 Pydantic 스키마를 사용할 수 있습니다.

### 3.6. 노드 (Graph Nodes)

*   `planner_node(state: AgentState)`:
    *   `AgentState`를 입력받아 LLM을 사용하여 체계적인 실행 계획을 수립합니다.
    *   계획 수립 원칙 (상황 파악, 정보 수집, 실행 및 결과물 생성, 단계적 접근)을 따릅니다.
    *   `PlanSchema`를 사용하여 구조화된 계획을 생성하고, `plan`과 `current_step_index`를 업데이트합니다.
*   `executor_node(state: AgentState)`:
    *   현재 `AgentState`의 `current_step_index`에 해당하는 계획 단계를 실행합니다.
    *   각 단계마다 LLM에게 실행가(Executor) 역할을 부여하고, 주어진 지침에 따라 적절한 도구를 호출하도록 합니다.
    *   도구 실행 중 오류 발생 시, `list_project_structure` 등으로 상황을 파악하여 문제를 해결하려 시도합니다.

## 4. 워크플로우 (LangGraph)

*   `LLM.py`는 LangGraph의 `StateGraph`를 사용하여 에이전트 워크플로우를 정의합니다.
*   주요 노드는 `planner_node`와 `executor_node`입니다.
*   `planner_node`는 초기 계획을 생성하고, `executor_node`는 계획에 따라 순차적으로 작업을 실행합니다.
*   `MemorySaver`를 사용하여 워크플로우의 상태를 저장하고 재개할 수 있습니다.
*   `ToolNode`는 정의된 도구들을 효율적으로 실행하는 데 사용됩니다.

## 5. 사용 예시 (개념적)

```python
# LangGraph를 이용한 워크플로우 정의 예시 (LLM.py 내부 로직)

workflow = StateGraph(AgentState)

workflow.add_node("planner", planner_node)
workflow.add_node("executor", executor_node)

# 계획 수립 후 실행 시작
workflow.set_entry_point("planner")

# 각 단계 실행 후 다음 단계 결정 로직 (간략화)
# 실제 구현에서는 더 복잡한 조건 분기나 루프가 있을 수 있음
workflow.add_edge("planner", "executor")

# 실행 완료 조건 (예: 모든 계획 단계 완료)
# executor_node에서 END를 반환하도록 구현되어야 함
workflow.add_conditional_edges(
    "executor",
    lambda state: END if state["current_step_index"] >= len(state["plan"]) else "executor", # 다음 단계가 없으면 종료, 있으면 계속 실행
    # 또는 다른 노드로 분기하는 로직 추가 가능
)

app = workflow.compile()

# 실행
# inputs = {"messages": [HumanMessage(content="...사용자 요청...")]}
# result = app.invoke(inputs)
```

## 6. 향후 계획 및 개선 사항

*   **오류 처리 강화:** 각 도구 및 LLM 호출에 대한 보다 상세하고 견고한 오류 처리 메커니즘 구현.
*   **병렬 실행:** `executor_node`에서 독립적인 작업들을 병렬로 처리하여 효율성 증대.
*   **메모리 관리:** 장기적인 대화나 복잡한 작업 수행 시 LLM의 컨텍스트 창 관리를 위한 전략 도입.
*   **사용자 정의 도구 지원:** 사용자가 쉽게 새로운 도구를 추가하고 워크플로우에 통합할 수 있는 인터페이스 제공.
*   **테스트 자동화:** `LLM.py`의 각 기능 및 전체 워크플로우에 대한 자동화된 테스트 케이스 작성.
*   **MCP 연동 고도화:** MCP 기능을 활용한 멀티모달 처리 및 외부 시스템 연동 강화.
