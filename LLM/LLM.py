import os
import sys

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ìµœìš°ì„  ìˆœìœ„)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# 2. LangSmith ì„¤ì •ì„ ì½”ë“œ ë ˆë²¨ì—ì„œ ê°•ì œ (í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
# ì´ ì½”ë“œëŠ” ë‹¤ë¥¸ ì–´ë–¤ LangChain ëª¨ë“ˆì´ ì„í¬íŠ¸ë˜ê¸° ì „ì— ì‹¤í–‰ë˜ì–´ì•¼ í•¨
if os.environ.get("LANGCHAIN_API_KEY"):
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = os.environ.get("LANGCHAIN_PROJECT", "Graph-Test")
    print(f"âœ… LangSmith Tracing Enabled. Project: {os.environ['LANGCHAIN_PROJECT']}")
else:
    print("âš ï¸ LangSmith API Key not found. Tracing disabled.")

import subprocess
import time
import random
import logging
import json
import asyncio
from typing import TypedDict, Annotated, List, Literal, Optional, Dict, Any
from contextlib import AsyncExitStack

# LangChain ê´€ë ¨ ì„í¬íŠ¸ëŠ” í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ì—
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver 
from langchain_google_genai import ChatGoogleGenerativeAI
from google.api_core.exceptions import ResourceExhausted

# --- [ì„¤ì •] ---
class Config:
    MODEL_NAME = "gemini-2.5-flash-lite"
    MAX_RETRIES = 5
    LOG_LEVEL = logging.INFO
    MCP_CONFIG_PATH = "config/mcp.json"

class SchemaWarningFilter(logging.Filter):
    def filter(self, record):
        msg = record.getMessage()
        if "additionalProperties" in msg or "$schema" in msg:
            return False
        return True

logging.basicConfig(level=Config.LOG_LEVEL, format="%(asctime)s - %(levelname)s - %(message)s")
for handler in logging.getLogger().handlers:
    handler.addFilter(SchemaWarningFilter())
logger = logging.getLogger(__name__)

try:
    import google.genai._extra_utils
    google.genai._extra_utils._DEFAULT_MAX_REMOTE_CALLS_AFC = 100
except ImportError:
    pass

# --- [ìƒíƒœ ì •ì˜] ---
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    plan: List[str]
    current_step_index: int
    final_report: Optional[dict]

# --- [êµ¬ì¡°í™”ëœ ì¶œë ¥] ---
class PlanSchema(BaseModel):
    steps: List[str] = Field(description="ì‹¤í–‰í•  êµ¬ì²´ì ì¸ ë‹¨ê³„ë³„ ê³„íš ëª©ë¡ (ì½”ë“œ ì‘ì„± ì‹œ í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„ë¡œ ì„¸ë¶„í™” í•„ìˆ˜)")

class ReviewReport(BaseModel):
    status: Literal["SUCCESS", "FAILED"]
    summary: str

# --- [ë„êµ¬ ì •ì˜] ---
@tool
def log_reasoning(reasoning: str) -> str:
    """[ë„êµ¬] í˜„ì¬ ìƒíƒœì— ëŒ€í•œ ë¶„ì„, ìƒê°, ë˜ëŠ” ê²€í†  ë‚´ìš©ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    return f"ë¶„ì„ ë‚´ìš©ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤: {reasoning[:100]}..."

@tool
def generate_code_draft(code_snippet: str, description: str) -> str:
    """[ë„êµ¬] íŒŒì¼ì— ì €ì¥í•˜ê¸° ì „ì— ì½”ë“œ ì´ˆì•ˆì„ ìƒì„±í•˜ì—¬ ê²€í† í•©ë‹ˆë‹¤.
    ì‹¤ì œ íŒŒì¼ ì €ì¥ ë„êµ¬ê°€ ì•„ë‹ˆë©°, ìƒì„±ëœ ì½”ë“œë¥¼ ë©”ëª¨ë¦¬ì— ì ì‹œ ë³´ê´€í•˜ëŠ” ìš©ë„ì…ë‹ˆë‹¤.
    ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•œ í›„, ë°˜ë“œì‹œ write_code_to_fileì´ë‚˜ append_to_fileì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤."""
    return f"ì½”ë“œ ì´ˆì•ˆ ìƒì„±ë¨ ({len(code_snippet)} chars). ë‚´ìš©ì„ í™•ì¸í•˜ê³  ì´ìƒ ì—†ìœ¼ë©´ íŒŒì¼ì— ì €ì¥í•˜ì„¸ìš”."

@tool
def list_project_structure(root_path: str = ".", max_depth: int = 3) -> Dict[str, Any]:
    """[ë„êµ¬] í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡° ì¡°íšŒ"""
    ignore_dirs = {".git", ".venv", "__pycache__", "node_modules", ".DS_Store", ".pytest_cache"}
    items = []
    try:
        abs_root = os.path.abspath(root_path)
        for root, dirs, files in os.walk(abs_root):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            rel_path = os.path.relpath(root, abs_root)
            level = 0 if rel_path == "." else rel_path.count(os.sep) + 1
            if level > max_depth: continue
            items.append({"path": rel_path, "type": "directory", "name": os.path.basename(root) or root})
            if level < max_depth:
                for f in files:
                    if not f.startswith("."):
                        items.append({"path": os.path.join(rel_path, f), "type": "file", "name": f})
        return {"status": "success", "items": items}
    except Exception as e: return {"status": "error", "message": str(e)}

@tool
def read_file(file_path: str) -> Dict[str, Any]:
    """[ë„êµ¬] íŒŒì¼ ë‚´ìš© ì½ê¸° (ìµœëŒ€ 10,000ì)"""
    try:
        if not os.path.exists(file_path): return {"status": "error", "message": f"File not found: {file_path}"}
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        return {"status": "success", "content": content[:10000] + ("...truncated" if len(content) > 10000 else "")}
    except Exception as e: return {"status": "error", "message": str(e)}

@tool
def write_code_to_file(file_path: str, content: str) -> Dict[str, Any]:
    """[ë„êµ¬] íŒŒì¼ ì‘ì„±/ë®ì–´ì“°ê¸° (ìƒˆ íŒŒì¼ ìƒì„± ì‹œ ì‚¬ìš©)"""
    try:
        abs_path = os.path.abspath(file_path)
        os.makedirs(os.path.dirname(abs_path), exist_ok=True)
        with open(abs_path, "w", encoding="utf-8") as f:
            f.write(content)
        return {"status": "success", "message": f"Successfully wrote to {file_path}"}
    except Exception as e: return {"status": "error", "message": str(e)}

@tool
def append_to_file(file_path: str, content: str) -> Dict[str, Any]:
    """[ë„êµ¬] ê¸°ì¡´ íŒŒì¼ ëì— ë‚´ìš© ì¶”ê°€ (ê¸´ ì½”ë“œë¥¼ ë‚˜ëˆ„ì–´ ì‘ì„±í•  ë•Œ ì‚¬ìš©)"""
    try:
        if not os.path.exists(file_path): return {"status": "error", "message": f"File not found: {file_path}. Use write_code_to_file to create it first."}
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(content)
        return {"status": "success", "message": f"Successfully appended to {file_path}"}
    except Exception as e: return {"status": "error", "message": str(e)}

@tool
def execute_command(command: str) -> Dict[str, Any]:
    """[ë„êµ¬] ì‰˜ ëª…ë ¹ì–´ ì‹¤í–‰"""
    try:
        res = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=60)
        return {"status": "success" if res.returncode == 0 else "failed", "stdout": res.stdout, "stderr": res.stderr}
    except Exception as e: return {"status": "error", "message": str(e)}

base_tools = [log_reasoning, generate_code_draft, list_project_structure, read_file, write_code_to_file, append_to_file, execute_command]

# --- [ë§¤ë‹ˆì € í´ë˜ìŠ¤] ---
class MCPManager:
    def __init__(self):
        self.exit_stack = AsyncExitStack()
        self.mcp_tools = []

    async def initialize(self):
        try:
            from mcp import ClientSession, StdioServerParameters
            from mcp.client.stdio import stdio_client
            from langchain_mcp_adapters.tools import load_mcp_tools
        except ImportError: return

        if not os.path.exists(Config.MCP_CONFIG_PATH): return
        try:
            with open(Config.MCP_CONFIG_PATH, "r") as f: config = json.load(f)
            for name, server_config in config.get("mcpServers", {}).items():
                if not server_config.get("command"): continue
                logger.info(f"Connecting to MCP: {name}")
                args = [os.path.expandvars(arg) for arg in server_config.get("args", [])]
                params = StdioServerParameters(command=server_config["command"], args=args, env=server_config.get("env"))
                transport = await self.exit_stack.enter_async_context(stdio_client(params))
                client = await self.exit_stack.enter_async_context(ClientSession(transport[0], transport[1]))
                await client.initialize()
                tools = await load_mcp_tools(client)
                self.mcp_tools.extend(tools)
        except Exception as e: logger.error(f"MCP Error: {e}")

    async def cleanup(self): await self.exit_stack.aclose()

class LLMManager:
    def __init__(self):
        self.llm = None
        self.llm_with_tools = None

    def initialize(self, extra_tools: List[Any] = None):
        if "GOOGLE_API_KEY" not in os.environ:
            logger.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        self.llm = ChatGoogleGenerativeAI(model=Config.MODEL_NAME, temperature=0)
        all_tools = base_tools + (extra_tools if extra_tools else [])
        self.llm_with_tools = self.llm.bind_tools(all_tools)

    async def invoke(self, mode: str, messages: List[BaseMessage], schema=None, **kwargs):
        model = self.llm_with_tools if mode == "tools" else self.llm
        if mode == "structured": model = self.llm.with_structured_output(schema)
        return await model.ainvoke(messages, **kwargs)

mcp_manager = MCPManager()
llm_manager = LLMManager()

# --- [ë…¸ë“œ ì •ì˜] ---

async def planner_node(state: AgentState):
    logger.info("ğŸ“… [Planner] ê³„íš ìˆ˜ë¦½...")
    prompt = """ë‹¹ì‹ ì€ ìˆ˜ì„ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì²´ê³„ì ì¸ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì‹­ì‹œì˜¤.

[ê³„íš ìˆ˜ë¦½ ì›ì¹™]
1. **ìƒí™© íŒŒì•…:** ì‘ì—… í™˜ê²½(íŒŒì¼ êµ¬ì¡° ë“±)ì„ ë¨¼ì € í™•ì¸í•˜ëŠ” ë‹¨ê³„ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.
2. **ì •ë³´ ìˆ˜ì§‘:** í•„ìš”í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ê²€ìƒ‰ì´ë‚˜ íŒŒì¼ ì½ê¸°ë¥¼ í†µí•´ í™•ë³´í•˜ëŠ” ë‹¨ê³„ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.
3. **ì‹¤í–‰ ë° ê²°ê³¼ë¬¼ ìƒì„±:** ì‚¬ìš©ìì˜ ìš”ì²­ì´ êµ¬ì²´ì ì¸ ê²°ê³¼ë¬¼(ì½”ë“œ, ë¬¸ì„œ, íŒŒì¼ ë“±)ì„ ìš”êµ¬í•œë‹¤ë©´, ì´ë¥¼ ì‹¤ì œë¡œ ìƒì„±í•˜ê³  ì €ì¥í•˜ëŠ” ë‹¨ê³„ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì‹­ì‹œì˜¤.
4. **ë‹¨ê³„ì  ì ‘ê·¼:** ë³µì¡í•œ ì‘ì—…ì€ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë ¤ í•˜ì§€ ë§ê³ , ë…¼ë¦¬ì ì¸ ìˆœì„œì— ë”°ë¼ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì‹­ì‹œì˜¤. (ì˜ˆ: ë¼ˆëŒ€ ì‘ì„± -> ì„¸ë¶€ ë‚´ìš© ì¶”ê°€ -> ê²€ì¦)
"""
    messages = [SystemMessage(content=prompt)] + state["messages"]
    plan_data = await llm_manager.invoke("structured", messages, PlanSchema)
    plan_summary = "\n".join([f"{i+1}. {s}" for i, s in enumerate(plan_data.steps)])
    logger.info(f"ğŸ“‹ ìˆ˜ë¦½ëœ ê³„íš:\n{plan_summary}")
    
    return {
        "plan": plan_data.steps,
        "current_step_index": 0,
        "messages": [AIMessage(content=f"ì‘ì—… ê³„íšì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤:\n{plan_summary}")]
    }

async def executor_node(state: AgentState):
    idx = state["current_step_index"]
    plan = state["plan"]
    if idx >= len(plan): return {"messages": []}

    current_task = plan[idx]
    logger.info(f"âš™ï¸ [Executor] ë‹¨ê³„ {idx+1}/{len(plan)}: {current_task}")

    system_prompt = f"""ë‹¹ì‹ ì€ ì‹¤í–‰ê°€(Executor)ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„: {current_task}

[í–‰ë™ ì§€ì¹¨]
1. **ë„êµ¬ ì‚¬ìš© í•„ìˆ˜:** í…ìŠ¤íŠ¸ë¡œ ëŒ€ë‹µí•˜ëŠ” ëŒ€ì‹ , í˜„ì¬ ë‹¨ê³„ë¥¼ ì™„ìˆ˜í•˜ê¸° ìœ„í•´ ê°€ì¥ ì ì ˆí•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì‹­ì‹œì˜¤.
2. **ê²°ê³¼ë¬¼ ì¤‘ì‹¬:** ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²°ê³¼ê°€ ìˆë‹¤ë©´, ë‹¨ìˆœí•œ ê³„íšì´ë‚˜ ìƒê°(`log_reasoning`)ì— ê·¸ì¹˜ì§€ ë§ê³ , `write_code_to_file` ë“±ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê²°ê³¼ë¬¼ì„ ë§Œë“¤ì–´ë‚´ì‹­ì‹œì˜¤.
3. **ë¬¸ì œ í•´ê²°:** ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜(ì˜ˆ: íŒŒì¼ ì—†ìŒ)ê°€ ë°œìƒí•˜ë©´, ì¦‰ì‹œ ë©ˆì¶”ì§€ ë§ê³  ìƒí™©ì„ íŒŒì•…(`list_project_structure` ë“±)í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹­ì‹œì˜¤.
4. **ë¯¸ì™„ì„± ê¸ˆì§€:** ì½”ë“œë‚˜ ë¬¸ì„œë¥¼ ì‘ì„±í•  ë•ŒëŠ” `TODO`ë‚˜ ë¹ˆì¹¸ìœ¼ë¡œ ë‚¨ê²¨ë‘ì§€ ë§ê³ , ë¬¸ë§¥ì— ë§ëŠ” ë‚´ìš©ì„ ì¶©ì‹¤íˆ ì±„ì›Œ ë„£ìœ¼ì‹­ì‹œì˜¤.
"""
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = await llm_manager.invoke("tools", messages, tool_choice="any")
    return {"messages": [response]}

async def update_step_node(state: AgentState):
    return {"current_step_index": state["current_step_index"] + 1}

async def reporter_node(state: AgentState):
    logger.info("ğŸ“ [Reporter] ê²°ê³¼ ë³´ê³ ...")
    messages = state["messages"] + [HumanMessage(content="ì‘ì—… ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ ë³´ê³ í•´ì¤˜.")]
    report = await llm_manager.invoke("structured", messages, ReviewReport)
    return {"final_report": report.model_dump()}

# --- [ê·¸ë˜í”„ êµ¬ì„±] ---

async def async_main():
    if "GOOGLE_API_KEY" not in os.environ:
        os.environ["GOOGLE_API_KEY"] = input("ğŸ”‘ Google API Key: ").strip()

    await mcp_manager.initialize()
    llm_manager.initialize(extra_tools=mcp_manager.mcp_tools)

    if not llm_manager.llm: return

    # ì²´í¬í¬ì¸í„° ì´ˆê¸°í™”
    memory = MemorySaver()

    workflow = StateGraph(AgentState)
    workflow.add_node("planner", planner_node)
    workflow.add_node("executor", executor_node)
    workflow.add_node("tools", ToolNode(base_tools + mcp_manager.mcp_tools))
    workflow.add_node("update_step", update_step_node)
    workflow.add_node("reporter", reporter_node)

    workflow.set_entry_point("planner")
    workflow.add_edge("planner", "executor")
    
    def route_executor(state):
        if state["messages"][-1].tool_calls: return "tools"
        return "update_step"

    workflow.add_conditional_edges("executor", route_executor, {"tools": "tools", "update_step": "update_step"})
    workflow.add_edge("tools", "update_step")
    workflow.add_conditional_edges("update_step", lambda x: "executor" if x["current_step_index"] < len(x["plan"]) else "reporter", {"executor": "executor", "reporter": "reporter"})
    workflow.add_edge("reporter", END)

    # ì²´í¬í¬ì¸í„°ë¥¼ í¬í•¨í•˜ì—¬ ê·¸ë˜í”„ ì»´íŒŒì¼
    app = workflow.compile(checkpointer=memory)
    
    user_input = input(">>> ì‘ì—… ì§€ì‹œ: ") or "í”„ë¡œì íŠ¸ë¥¼ ë¶„ì„í•´ì¤˜."
    
    # í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  thread_id ìƒì„± (ì‹ë³„ ê°€ëŠ¥í•˜ê²Œ)
    current_time = time.strftime("%Y%m%d_%H%M%S")
    thread_id = f"session_{current_time}"
    logger.info(f"ğŸš€ ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘ - Thread ID: {thread_id}")
    
    # ìŠ¤ë ˆë“œ ì„¤ì •
    config = {"configurable": {"thread_id": thread_id}}

    async for event in app.astream({"messages": [HumanMessage(content=user_input)], "plan": [], "current_step_index": 0}, config=config):
        for key, value in event.items():
            if "messages" in value:
                m = value["messages"][-1]
                if isinstance(m, AIMessage):
                    if m.tool_calls:
                        for tc in m.tool_calls: print(f"ğŸ› ï¸ [Tool]: {tc['name']}")
                    elif m.content: print(f"ğŸ¤– [AI]: {m.content[:100]}...")
                elif isinstance(m, ToolMessage): print(f"âš¡ [Result]: {str(m.content)[:50]}...")
            if "final_report" in value: print(f"\nâœ… [ì™„ë£Œ]: {value['final_report']['summary']}")

    await mcp_manager.cleanup()

if __name__ == "__main__":
    asyncio.run(async_main())
